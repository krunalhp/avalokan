{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNk4TlbzkRdtGNa2PZuqFDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krunalhp/avalokan/blob/main/VideoAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Function to add activity labels and track movement\n",
        "def annotate_frame(frame, person_id, activity_label, bbox):\n",
        "    # Set font and text color\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1\n",
        "    color = (0, 255, 0)  # Green\n",
        "    thickness = 2\n",
        "\n",
        "    # Draw bounding box around detected person\n",
        "    x, y, w, h = bbox\n",
        "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "\n",
        "    # Add ID and activity label above bounding box\n",
        "    label = f\"ID: {person_id}, Activity: {activity_label}\"\n",
        "    text_size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
        "    text_x = x + (w - text_size[0]) // 2\n",
        "    text_y = y - 10\n",
        "    cv2.putText(frame, label, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return frame\n",
        "\n",
        "# Function to process and annotate the video with people detection and activity\n",
        "def analyze_local_video(video_path):\n",
        "    # Open the video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Unable to open video.\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for output video\n",
        "\n",
        "    # Set output video path\n",
        "    output_video_path = \"/content/sample_data/output_video_with_activity.mp4\"\n",
        "\n",
        "    # Initialize the video writer\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Create a HOG person detector (pre-trained model)\n",
        "    hog = cv2.HOGDescriptor()\n",
        "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "\n",
        "    person_id = 0  # ID for the people detected\n",
        "    prev_positions = []  # Store previous positions for activity tracking\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break  # End of video\n",
        "\n",
        "        # Detect people in the frame\n",
        "        boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8), padding=(8, 8), scale=1.05)\n",
        "\n",
        "        annotated_frame = frame.copy()  # Make sure annotated_frame is defined\n",
        "\n",
        "        # Iterate over the detected people and track their movement\n",
        "        for bbox in boxes:\n",
        "            x, y, w, h = bbox\n",
        "\n",
        "            # Track movement to classify activity\n",
        "            centroid = (x + w // 2, y + h // 2)\n",
        "\n",
        "            # For simplicity, we'll assume each detected box represents a unique person\n",
        "            matched = False\n",
        "            for i, (prev_centroid, prev_bbox) in enumerate(prev_positions):\n",
        "                prev_x, prev_y, prev_w, prev_h = prev_bbox\n",
        "                # Check if bounding boxes are close (indicating the same person)\n",
        "                if abs(x - prev_x) < 100 and abs(y - prev_y) < 100:  # Threshold for re-identifying the person\n",
        "                    # Calculate movement speed (delta in centroid)\n",
        "                    dx = centroid[0] - prev_centroid[0]\n",
        "                    dy = centroid[1] - prev_centroid[1]\n",
        "                    speed = np.sqrt(dx**2 + dy**2)\n",
        "\n",
        "                    # Define activity based on speed (simple thresholding)\n",
        "                    if speed > 10:\n",
        "                        activity_label = \"Running\"\n",
        "                    else:\n",
        "                        activity_label = \"Walking\"\n",
        "\n",
        "                    prev_positions[i] = [centroid, bbox]  # Update the tracked position\n",
        "                    matched = True\n",
        "                    break\n",
        "\n",
        "            if not matched:\n",
        "                # If no match, treat this as a new person\n",
        "                prev_positions.append([centroid, bbox])\n",
        "                activity_label = \"Walking\"  # Default activity for new person\n",
        "                person_id += 1  # Increment person ID\n",
        "\n",
        "            # Annotate the frame with bounding box and activity\n",
        "            annotated_frame = annotate_frame(annotated_frame, person_id, activity_label, bbox)\n",
        "\n",
        "        # Write the frame to the output video\n",
        "        out.write(annotated_frame)\n",
        "\n",
        "    # Release the resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Processed video saved as {output_video_path}\")\n",
        "\n",
        "# Path to the local video file\n",
        "video_path = \"/content/sample_data/cctv.mp4\"  # Path to your local video file\n",
        "\n",
        "if os.path.exists(video_path):\n",
        "    analyze_local_video(video_path)\n",
        "else:\n",
        "    print(f\"Video file not found: {video_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOUinsQ3l2oB",
        "outputId": "b614b75a-6807-40cc-d640-cf58385d6f52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed video saved as /content/sample_data/output_video_with_activity.mp4\n"
          ]
        }
      ]
    }
  ]
}